{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Weather_cleaning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOSNLsKU5Wlgj6xQE++2p32"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"b6rAHGJuVWZB","executionInfo":{"status":"ok","timestamp":1599629279532,"user_tz":-600,"elapsed":4880,"user":{"displayName":"Nebojsa Ajdarevic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwSJyxQjlCx1uDhi-FIcMDxmucq3wFPRvFtZ5W=s64","userId":"04450095322838833303"}},"outputId":"e413549c-ec35-40dd-94a3-99008f8cc5a4","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!pip install wwo-hist"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting wwo-hist\n","  Downloading https://files.pythonhosted.org/packages/5a/b4/19a4d6a0d131567cf4b2ffa3758710d867f7d7d3f0c6f94bd63fadf1d02a/wwo_hist-0.0.5-py3-none-any.whl\n","Installing collected packages: wwo-hist\n","Successfully installed wwo-hist-0.0.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7Xyjx2L8VKf8"},"source":["5a38e33a56d049149ab24946200809"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yYJOvmofWknT","executionInfo":{"status":"ok","timestamp":1599629647930,"user_tz":-600,"elapsed":48295,"user":{"displayName":"Nebojsa Ajdarevic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwSJyxQjlCx1uDhi-FIcMDxmucq3wFPRvFtZ5W=s64","userId":"04450095322838833303"}},"outputId":"905a10e8-686d-4d89-b3cc-7da17d1cdffc","colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["#mount to google drive first\n","import io\n","from google.colab import drive  \n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","4/3wHp7wvqpm-FK6rQGzNUg1R4eeBSzGa4mYRJo6U3llOzWN7ox9EyVgY\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FrF7lRIsWYIE"},"source":["import os\n","path = \"/content/drive/My Drive/UNIVERSITY/Sem 4 - 2020/IFN - 704/Data/\"\n","\n","os.chdir(path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EB76m797hwU8"},"source":["Had to update the source code below to Download API data"]},{"cell_type":"code","metadata":{"id":"fdgQQTeLhuYq"},"source":["\"\"\n","\n","import urllib\n","import urllib.request\n","import urllib.parse\n","import json\n","import pandas as pd\n","from datetime import datetime\n","import os\n","\n","\n","##################################\n","# function to unnest json for each month\n","def extract_monthly_data(data):\n","    num_days = len(data)\n","    # initialize df_month to store return data\n","    df_month = pd.DataFrame()\n","    for i in range(num_days):\n","        # extract this day\n","        d = data[i]\n","        # astronomy data is the same for the whole day\n","        astr_df = pd.DataFrame(d['astronomy'])\n","        # hourly data; temperature for each hour of the day\n","        hourly_df = pd.DataFrame(d['hourly'])\n","        # this wanted_key will be duplicated and use 'ffill' to fill up the NAs\n","        wanted_keys = ['date', 'maxtempC', 'mintempC', 'totalSnow_cm', 'sunHour', 'uvIndex']  # The keys you want\n","        subset_d = dict((k, d[k]) for k in wanted_keys if k in d)\n","        this_df = pd.DataFrame(subset_d, index=[0])\n","        df = pd.concat([this_df.reset_index(drop=True), astr_df], axis=1)\n","        # concat selected astonomy columns with hourly data\n","        df = pd.concat([df, hourly_df], axis=1)\n","        df = df.fillna(method='ffill')\n","        # make date_time columm to proper format\n","        # fill leading zero for hours to 4 digits (0000-2400 hr)\n","        df['time'] = df['time'].apply(lambda x: x.zfill(4))\n","        # keep only first 2 digit (00-24 hr) \n","        df['time'] = df['time'].str[:2]\n","        # convert to pandas datetime\n","        df['date_time'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n","        # keep only interested columns\n","        col_to_keep = ['date_time', 'maxtempC', 'mintempC', 'totalSnow_cm', 'sunHour', 'uvIndex',\n","                       'moon_illumination', 'moonrise', 'moonset', 'sunrise', 'sunset',\n","                       'DewPointC', 'FeelsLikeC', 'HeatIndexC', 'WindChillC', 'WindGustKmph',\n","                       'cloudcover', 'humidity', 'precipMM', 'pressure', 'tempC', 'visibility',\n","                       'winddirDegree', 'windspeedKmph']\n","        df = df[col_to_keep]\n","        df = df.loc[:,~df.columns.duplicated()]\n","        df_month = pd.concat([df_month, df])\n","    return (df_month)\n","\n","\n","##################################\n","# function to retrive data by date range and location\n","# default frequency = 1 hr\n","# each month costs 1 request (free trial 500 requests/key, as of 30-May-2019)\n","def retrieve_this_location(api_key, location, start_date, end_date, frequency, response_cache_path):\n","    start_time = datetime.now()\n","\n","    # create list of first day of month for range between start and end dates non-inclusive (open)\n","    list_mon_begin = pd.date_range(start_date, end_date, freq='MS', closed='right')\n","    # convert to Series and add start_date at beginning\n","    list_mon_begin = pd.concat([pd.Series(pd.to_datetime(start_date)), pd.Series(list_mon_begin)], ignore_index=True)\n","\n","    # create list of month end dates for range between start and end dates non-inclusive (open)\n","    list_mon_end = pd.date_range(start_date, end_date, freq='M', closed='left')\n","    # convert to Series and add end_date at end\n","    list_mon_end = pd.concat([pd.Series(list_mon_end), pd.Series(pd.to_datetime(end_date))], ignore_index=True)\n","\n","    # count number of months to be retrieved\n","    total_months = len(list_mon_begin)\n","\n","    # initialize df_hist to store return data\n","    df_hist = pd.DataFrame()\n","    for m in range(total_months):\n","        start_d = str(list_mon_begin[m])[:10]\n","        end_d = str(list_mon_end[m])[:10]\n","        file_path = f'{response_cache_path}/{location}_{start_d}_{end_d}'\n","        if response_cache_path and os.path.exists(file_path):\n","            print('Reading cached data for ' + location + ': from ' + start_d + ' to ' + end_d)\n","            with open(f'{response_cache_path}/{location}_{start_d}_{end_d}', 'r') as f:\n","                json_data = json.load(f)\n","        else:\n","            print('Currently retrieving data for ' + location + ': from ' + start_d + ' to ' + end_d)\n","            url_page = 'http://api.worldweatheronline.com/premium/v1/past-weather.ashx?key=' + api_key + '&q=' + location + '&format=json&date=' + start_d + '&enddate=' + end_d + '&tp=' + str(\n","                frequency)\n","            json_page = urllib.request.urlopen(url_page, timeout=10)\n","            json_data = json.loads(json_page.read().decode())\n","\n","        if response_cache_path:\n","            with open(f'{response_cache_path}/{location}_{start_d}_{end_d}', 'w') as f:\n","                json.dump(json_data, f)\n","        data = json_data['data']['weather']\n","        # call function to extract json object\n","        df_this_month = extract_monthly_data(data)\n","        df_this_month['location'] = location\n","        df_hist = pd.concat([df_hist, df_this_month])\n","\n","        time_elapsed = datetime.now() - start_time\n","        print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))\n","    return (df_hist)\n","\n","\n","##################################\n","# main function to retrive the data by location list\n","def retrieve_hist_data(api_key, location_list, start_date, end_date, frequency, location_label=False, export_csv=True,\n","                       store_df=False, response_cache_path=None):\n","    result_list = []\n","    for location in location_list:\n","        print('\\n\\nRetrieving weather data for ' + location + '\\n\\n')\n","        df_this_city = retrieve_this_location(api_key, location, start_date, end_date, frequency, response_cache_path)\n","\n","        if (location_label == True):\n","            # add city name as prefix to the colnames\n","            df_this_city = df_this_city.add_prefix(location + '_')\n","            df_this_city.columns.values[0] = 'date_time'\n","\n","        if (export_csv == True):\n","            df_this_city.to_csv('./' + location + '.csv', header=True, index=False)\n","            print('\\n\\nexport ' + location + ' completed!\\n\\n')\n","\n","        if (store_df == True):\n","            # save result as object in the work space\n","            result_list.append(df_this_city)\n","\n","    return (result_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tRdCF2LcVL1y","executionInfo":{"status":"ok","timestamp":1599631457726,"user_tz":-600,"elapsed":54996,"user":{"displayName":"Nebojsa Ajdarevic","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwSJyxQjlCx1uDhi-FIcMDxmucq3wFPRvFtZ5W=s64","userId":"04450095322838833303"}},"outputId":"038df354-5ec8-4518-8b7d-48404e71926d","colab":{"base_uri":"https://localhost:8080/","height":595}},"source":["#retireve weather data from the API\n","\n","frequency = 1\n","start_date = '01-JAN-2019'\n","end_date = '31-DEC-2019'\n","api_key = '5a38e33a56d049149ab24946200809'\n","location_list = ['brisbane']\n","hist_weather_data = retrieve_hist_data(api_key,\n","                                location_list,\n","                                start_date,\n","                                end_date,\n","                                frequency,\n","                                location_label = False,\n","                                export_csv = True,\n","                                store_df = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","Retrieving weather data for brisbane\n","\n","\n","Currently retrieving data for brisbane: from 2019-01-01 to 2019-01-31\n","Time elapsed (hh:mm:ss.ms) 0:00:04.669650\n","Currently retrieving data for brisbane: from 2019-02-01 to 2019-02-28\n","Time elapsed (hh:mm:ss.ms) 0:00:09.116890\n","Currently retrieving data for brisbane: from 2019-03-01 to 2019-03-31\n","Time elapsed (hh:mm:ss.ms) 0:00:13.829990\n","Currently retrieving data for brisbane: from 2019-04-01 to 2019-04-30\n","Time elapsed (hh:mm:ss.ms) 0:00:18.389643\n","Currently retrieving data for brisbane: from 2019-05-01 to 2019-05-31\n","Time elapsed (hh:mm:ss.ms) 0:00:22.585554\n","Currently retrieving data for brisbane: from 2019-06-01 to 2019-06-30\n","Time elapsed (hh:mm:ss.ms) 0:00:26.673838\n","Currently retrieving data for brisbane: from 2019-07-01 to 2019-07-31\n","Time elapsed (hh:mm:ss.ms) 0:00:31.380602\n","Currently retrieving data for brisbane: from 2019-08-01 to 2019-08-31\n","Time elapsed (hh:mm:ss.ms) 0:00:36.086340\n","Currently retrieving data for brisbane: from 2019-09-01 to 2019-09-30\n","Time elapsed (hh:mm:ss.ms) 0:00:40.526925\n","Currently retrieving data for brisbane: from 2019-10-01 to 2019-10-31\n","Time elapsed (hh:mm:ss.ms) 0:00:45.506743\n","Currently retrieving data for brisbane: from 2019-11-01 to 2019-11-30\n","Time elapsed (hh:mm:ss.ms) 0:00:49.611284\n","Currently retrieving data for brisbane: from 2019-12-01 to 2019-12-31\n","Time elapsed (hh:mm:ss.ms) 0:00:53.768765\n","\n","\n","export brisbane completed!\n","\n","\n"],"name":"stdout"}]}]}